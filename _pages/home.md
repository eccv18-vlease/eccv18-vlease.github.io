---
layout: project
urltitle:  "Visual Learning and Embodied Agents in Simulation Environments"
title: "Visual Learning and Embodied Agents in Simulation Environments"
categories: eccv, munich, germany, workshop, computer vision, embodied agents, visual learning, simulation environments, robotics, visual navigation, machine learning, natural language processing, reinforcement learning
permalink: /
favicon: /static/img/embodiedqa/favicon.png
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Visual Learning and Embodied Agents in Simulation Environments</h1></center>
    <center><h2>ECCV 2018 Workshop, Munich, Germany</h2></center>
    <center><span style="color:#e74c3c;font-weight:400;">
      Sunday, 9th September, 08:45 AM to 06:00 PM, Room: TBD
    </span></center>
  </div>
</div>

<hr>

<div class="row" id="intro">
  <div class="col-md-12">
    <img src="{{ "/static/img/splash.png" | prepend:site.baseurl }}">
    <p> Image credit: [2, 28, 12, 11, 15-21, 26]</p>
  </div>
</div>

<br>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      Simulation environments are having a profound impact on computer vision and
      artificial intelligence (AI) research. Synthetic environments can be used to generate
      unlimited cheap, labeled data for training data-hungry <span style="font-weight:500;">visual learning</span> algorithms
      for perception tasks such as 3D pose estimation [1, 2], object detection and
      recognition [3, 4], semantic segmentation [5], 3D reconstruction [6-9], intuitive
      physics modeling [10-13] and text localization [14]. In addition, visually-realistic
      simulation environments designed for <span style="font-weight:500;">embodied agents</span> [15-21] have reignited interest
      in high-level AI tasks such as visual navigation [22, 23], natural language
      instruction following [20, 24, 25] and embodied question answering [26, 27]. This
      workshop will bring together researchers from computer vision, machine learning,
      natural language processing and robotics to examine the challenges and
      opportunities in this rapidly developing area - using simulation environments to
      develop intelligent embodied agents and other vision-based systems.
    </p>
  </div>
</div> <br>   

<div class="row">
  <div class="col-xs-12">
    <h2>Call for Papers and Demos</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      We invite high-quality paper submissions, optionally with a live demo. Accepted papers
      will be presented during joint poster/demo sessions, with exceptional submissions selected
      for spotlight oral presentation.
    </p>
    <p>
      Submissions must indicate if a poster-and-demo or a poster-only slot is requested. If a demo
      is requested, a short description of the demo and any equipment requirements must be provided.
      Note that demos should provide workshop participants with an opportunity to interact with
      simulation environments, algorithms, and agents in order to better understand the strengths
      and limitations of current work. Non-interactive visual displays, e.g. video or slide
      presentations will be rejected as demos.
    </p>
    <p>
      Submissions are divided into two tracks and dual submission of a paper to both is prohibited:
    </p><br>

    <h3><span style="font-weight:500;">Visual Learning Track</span></h3><br>
    <p><span style="font-weight:500;">Topics:</span> We welcome work focused on the use of synthetic
      data in broad computer vision tasks including but not limited to 3D pose estimation, object
      recognition, object detection, semantic segmentation, text localization, single-image 3D reconstruction,
      indoor/outdoor scene understanding, single-image VQA, and intuitive physics. Paper topics may include but are not limited to:
      <ul>
        <li>Use of synthetic data in visual learning tasks</li>
        <li>Novel computer vision tasks using synthetic data</li>
        <li>Learning synthetic data generation protocols</li>
        <li>Domain adaptation from synthetic data to the real world</li>
      </ul>
    </p>
    <p> <span style="font-weight:500;">Submission:</span> All Visual Learning Track submissions will be handled
      electronically via the workshop <span style="color:#1a1aff;font-weight:400;"><a href="https://cmt3.research.microsoft.com/VLEASE2018">CMT website</a></span>.
      Submissions to the Visual Learning Track should be between 4 and 14 pages in the <span style="color:#1a1aff;font-weight:400;"><a href="https://eccv2018.org/wp-content/uploads/2018/02/eccv2018kit.zip">ECCV format</a></span>, excluding
      references, acknowledgements, and supplementary materials. All the accepted submissions will be published
      separately from the main conference in the post-proceedings by default, though authors could indicate
      explicitly if they want to opt out the post-proceedings. Dual submission is allowed, but must be explicitly
      stated at the time of submission and will not be included in the post-proceedings. Reviewing will be double-blind.
      Each submission will be reviewed by at least three reviewers for originality, significance, clarity, soundness,
      relevance and technical contents. Papers that are not blind, or have the wrong format, or have either less
      than 4 pages or more than 14 pages (excluding references) will be rejected without review.  Please contact
      <span style="color:#1a1aff;font-weight:400;"><a href="mailto:vlease2018.visuallearning@gmail.com">vlease2018.visuallearning@gmail.com</a></span>
      for any concerns.
    </p><br>

    <h3><span style="font-weight:500;">Embodied Agents Track</span></h3><br>
    <p><span style="font-weight:500;">Topics:</span> We invite extended abstracts for work on embodied agents operating in simulation environments including reinforcement learning and approaches that use mapping and planning. Paper topics may include but are not limited to:
      <ul>
        <li>Novel datasets / simulators / tasks for embodied agents</li>
        <li>Language-based command of embodied agents, including embodied question answering and / or dialog</li>
        <li>Photo-realistic simulations from reconstructed point clouds / 3D meshes</li>
        <li>Simulating interactions with objects, other agents, and environmental changes</li>
        <li>Domain adaptation for embodied agents</li>
      </ul>
    </p>
    <p id="dates"> <span style="font-weight:500;">Submission:</span> For the Embodied Agents Track, we encourage 6
      page submissions excluding references, acknowledgements, and supplementary material. The submission should be in the <span style="color:#1a1aff;font-weight:400;"><a href="https://eccv2018.org/wp-content/uploads/2018/02/eccv2018kit.zip">ECCV format</a></span>.
      Reviewing will be single blind. Accepted extended abstracts will be made publicly
      available as non-archival reports, allowing future submissions to archival conferences or journals.
      We also welcome published papers that are within the scope of the workshop (without re-formatting), including papers
      from the main ECCV conference.  Please submit
      your Embodied Agents Track paper to the following address by the deadline:
      <span style="color:#1a1aff;font-weight:400;"><a href="mailto:embodiedagents@gmail.com">embodiedagents@gmail.com</a></span>.
      Please mention in your email if your submission has already been accepted for publication (and the name of the conference).
    </p>

  </div>

</div><br>

<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline
          <span style="color:#e74c3c;font-weight:400;">(extended!)</span></td>
          <td>July 31st, 2018</td>
        </tr>
        <tr>
          <td>Final Decisions</td>
          <td>August 8th, 2018</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>September 9th, 2018</td>
        </tr>
      </tbody>
    </table>
    <!-- <p>15th June : Submission Deadline</p>
    <p>16h July : Acceptance Notification</p>
    <p>1st August : Camera-ready deadline</p>
    <p>9th Septmeber : Workshop date</p> -->
    <!-- <ul>
      <li>15th June : Submission Deadline</li>
      <li>16h July : Acceptance Notification</li>
      <li>1st August : Camera-ready deadline</li>
      <li>9th Septmeber : Workshop date</li>
    </ul> -->
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <ul>
      <li>08:45 AM : Welcome and Introduction</li>
      <li>09:00 AM : Invited Talk: Peter Welinder</li>
      <li>09:25 AM : Invited Talk: Alan Yuille</li>
      <li>09:50 AM : Invited Talk: Sanja Fidler</li>
      <li>10:15 AM : Coffee + Posters/Demos (Both Tracks)</li>
      <li>11:00 AM : Invited Talk: Boqing Gong</li>
      <li>11:25 AM : Invited Talk: Lawson Wong</li>
      <li>11:50 AM : Poster Spotlight Presentations (4 × 5 min)</li>
      <li>12:10 PM : Lunch</li>
      <li>01:30 PM : Invited Talk: Abhinav Gupta</li>
      <li>01:55 PM : Invited Talk: Anton van den Hengel</li>
      <li>02:20 PM : Invited Talk: Vladlen Koltun</li>
      <li>02:45 PM : Coffee + Posters/Demos (Both Tracks)</li>
      <li>03:30 PM : Invited Talk: Raia Hadsell</li>
      <li>03:55 PM : Invited Talk: Dhruv Batra</li>
      <li>04:20 PM : Invited Talk: Jitendra Malik</li>
      <li>04:45 PM : Panel Discussion</li>
      <li id="speakers">05:30 PM : Closing Remarks</li>
    </ul>
  </div>
</div>

<br>
<div class="row">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<br>
<div class="row">
  <div class="col-md-12">
  <h3><span style="font-weight:500;">Embodied Agents Track</span></h3><br>
    <ol>
      <li>
        Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments -
        <i>Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhauf, Ian Reid, Stephen Gould, Anton van den Hengel</i> &nbsp;
        <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/vln.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
      </li>

      <li>
        Grounding Natural Language Instructions to Semantic Goal Representations for Abstraction and Generalization -
        <i>Dilip Arumugam*, Siddharth Karamcheti*, Nakul Gopalan, Edward C. Williams, Mina Rhee Lawson, L.S. Wong, Stefanie Tellex</i> &nbsp;
        <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/grounding-nl-instructions.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
      </li>

      <li>
        Following High-level Navigation Instructions on a Simulated Quadcopter with Imitation Learning -
        <i>Valts Blukis, Nataly Brukhim, Andrew Bennett, Ross A. Knepper, Yoav Artzi</i> &nbsp;
        <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/simulated-quadcopter.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
      </li>

      <li>
        Speaker-Follower Models for Vision-and-Language Navigation -
        <i>Daniel Fried*, Ronghang Hu*, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein**, Trevor Darrell**</i> &nbsp;
        <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/speaker-follower.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
      </li>

      <li>
        Deep Reinforcement Learning of an Agent in a Modern 3D Video Game -
        <i>Samuel Arzt, Gerhard Mitterlechner, Markus Tatzgern, and Thomas Stutz</i> &nbsp;
        <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/drl-video-game.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
      </li>

      <li>
        Neural Modular Control for Embodied Question Answering -
        <i>Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra</i> &nbsp;
        <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/eqa-modular.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
      </li>

      <li>
        Building Generalizable Agents with a Realistic and Rich 3D Environment -
        <i>Yi Wu, Yuxin Wu, Georgia Gkioxari, Yuandong Tian, Aviv Tamar, Stuart Russell</i> &nbsp;
        <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/house3D.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
      </li>

      <li>
        Learning a Semantic Prior for Guided Navigation -
        <i>Yi Wu, Yuxin Wu, Georgia Gkioxari, Yuandong Tian</i> &nbsp;
        <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/learning-a-semantic-prior.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
      </li>

      <li>
        Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation -
        <i>Xin Wang*, Wenhan Xiong**, Hongmin Wang, William Yang Wang</i>
        <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/look-before-you-leap.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
      </li>

      <li>
        Talk the Walk: Navigating New York City through Grounded Dialogue -
        <i>Harm de Vries, Kurt Shuster, Dhruv Batra, Devi Parikh, Jason Weston & Douwe Kiela</i>
        <span style="color:#1a1aff;font-weight:400;">[<a href="{{ "/static/papers/talk-walk-navigating.pdf" | prepend:site.baseurl }}">pdf</a>]</span>
      </li>
    </ol>

    *, ** Equal contribution
  </div>
</div>

<br>
<div class="row">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/jitendra.png" | prepend:site.baseurl }}">
    <p><b>Jitendra Malik</b> is the Arthur J. Chick Professor in the Department of Electrical Engineering and Computer Sciences at UC Berkeley. His research group has worked on many different topics in computer vision, computational modeling of human vision and computer graphics. Several well-known concepts and algorithms arose in this research, such as normalized cuts, high dynamic range imaging and R-CNN. He has mentored more than 50 PhD students and postdoctoral fellows. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://people.eecs.berkeley.edu/~malik/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/vladlen.png" | prepend:site.baseurl }}">
    <p><b>Vladlen Koltun</b> is a Senior Principal Researcher and the director of the Intelligent Systems Lab at Intel. The lab is devoted to high-impact basic research on intelligent systems. Previously, he has been a Senior Research Scientist at Adobe Research and an Assistant Professor at Stanford where his theoretical research was recognized with the National Science Foundation (NSF) CAREER Award (2006) and the Sloan Research Fellowship (2007). &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://vladlen.info/">Webpage]</a></span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/dhruv.png" | prepend:site.baseurl }}">
    <p><b>Dhruv Batra</b> is an Assistant Professor in the School of Interactive Computing at Georgia Tech and a Research Scientist at Facebook AI Research (FAIR). His research interests lie at the intersection of machine learning, computer vision, natural language processing, and AI. He is a recipient of numerous awards including the Office of Naval Research (ONR) Young Investigator Program (YIP) award (2016), two Google Faculty Research Awards (2013, 2015) and the Amazon Academic Research award (2016). &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://www.cc.gatech.edu/~parikh/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/raia.png" | prepend:site.baseurl }}">
    <p><b>Raia Hadsell</b>, a senior research scientist at DeepMind, has worked on deep learning and robotics problems for over 10 years. After completing a PhD with Yann LeCun at NYU, her research continued at Carnegie Mellon's Robotics Institute and SRI International, and in early 2014 she joined DeepMind in London to study artificial general intelligence. Her current research focuses on the challenge of continual learning for AI agents and robotic systems. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://raiahadsell.com/index.html">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/lawson.png" | prepend:site.baseurl }}">
    <p><b>Lawson Wong</b> is a postdoctoral fellow at Brown University, working with Stefanie Tellex. He completed his Ph.D. in 2016 at the Massachusetts Institute of Technology, advised by Leslie Pack Kaelbling and Tomás Lozano-Pérez. His current research focuses on acquiring, representing, and estimating knowledge about the world that an autonomous robot may find useful. He was awarded a AAAI Robotics Student Fellowship in 2015. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://cs.brown.edu/~lwong5/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/abhinav.png" | prepend:site.baseurl }}">
    <p><b>Abhinav Gupta</b> is an Assistant Professor in the Robotics Institute at Carnegie Mellon University (CMU). Prior to this, he was a post-doctoral fellow at CMU working with Alyosha Efros and Martial Hebert. His research interests include developing representations of the visual world, linking language and vision, and the relationships between objects and actions. He is a recipient of the PAMI Young Researcher award, the Bosch Young Faculty Fellowship and a Google Faculty Research Award. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://www.cs.cmu.edu/~abhinavg/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/anton.png" | prepend:site.baseurl }}">
    <p><b>Anton van den Hengel</b> is a Professor in the School of Computer Science at the University of Adelaide in Australia, the founding Director of the Australian Centre for Visual Technologies (ACVT), a Chief Investigator of the Australian Centre for Robotic Vision and a Program Leader in the Data 2 Decisions Cooperative Research Centre. He has won best paper at CVPR, published over 300 publications, had eight patents commercialized and founded two startups. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://cs.adelaide.edu.au/users/hengel/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/sanja.png" | prepend:site.baseurl }}">
    <p><b>Sanja Fidler</b> is an Assistant Professor at University of Toronto. Her main research interests are 2D and 3D object detection, particularly scalable multi-class detection, object segmentation and image labeling, and (3D) scene understanding. She is also interested in the interplay between language and vision. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://www.cs.utoronto.ca/~fidler/">Webpage</a>] </span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/boqing.png" | prepend:site.baseurl }}">
    <p><b>Boqing Gong</b> is a Principal Researcher at Tencent AI Lab in Seattle, working on machine learning and computer vision. Before joining Tencent, he was a tenure-track Assistant Professor in University of Central Florida (UCF). He received a Ph.D. in Computer Science from the University of Southern California, where his work was partially supported by the Viterbi Fellowship. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://boqinggong.info/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/alan.png" | prepend:site.baseurl }}">
    <p><b>Alan Yuille</b> is a Bloomberg Distinguished Professor of Cognitive Science and Computer Science at Johns Hopkins University. He directs the research group on Compositional Cognition, Vision, and Learning. He is affiliated with  the Center for Brains, Minds and Machines, and the NSF Expedition in Computing, Visual Cortex On Silicon. His research interests include computational models of vision, mathematical models of cognition, medical image analysis, and artificial intelligence and neural networks. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://www.cs.jhu.edu/~ayuille/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row"  id="organizers">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/peterw.png" | prepend:site.baseurl }}">
    <p><b>Peter Welinder</b> is a research scientist at OpenAI. He works on topics ranging from deep reinforcement learning and computer vision to robotics software/hardware and simulation/rendering. Previously he founded and managed the Machine Learning Team at Dropbox. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://www.linkedin.com/in/welinder/">Webpage</a>]</span></p>
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-2">
    <a href="http://www.panderson.me/">
      <img class="people-pic" src="{{ "/static/img/people/peter.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.panderson.me/">Peter Anderson</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://msavva.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/manolis.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://msavva.github.io/">Manolis Savva</a>
      <h6>Facebook AI Research, Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/angel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Eloquent Labs, Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://people.eecs.berkeley.edu/~sgupta/">
      <img class="people-pic" src="{{ "/static/img/people/saurabh.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://people.eecs.berkeley.edu/~sgupta/">Saurabh Gupta</a>
      <h6>UC Berkeley</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://cs.stanford.edu/~amirz/">
      <img class="people-pic" src="{{ "/static/img/people/amir.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://cs.stanford.edu/~amirz/">Amir R. Zamir</a>
      <h6>Stanford University, UC Berkeley</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cc.gatech.edu/~slee3191/">
      <img class="people-pic" src="{{ "/static/img/people/stefan.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cc.gatech.edu/~slee3191/">Stefan Lee</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>
</div>

<div class="row">
  <div class="col-xs-2">
    <a href="https://samyak-268.github.io">
      <img class="people-pic" src="{{ "/static/img/people/samyak.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://samyak-268.github.io">Samyak Datta</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://cs.stanford.edu/~ericyi/">
      <img class="people-pic" src="{{ "/static/img/people/li.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://cs.stanford.edu/~ericyi/">Li Yi</a>
      <h6>Stanford University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://cseweb.ucsd.edu/~haosu/">
      <img class="people-pic" src="{{ "/static/img/people/hao.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://cseweb.ucsd.edu/~haosu/">Hao Su</a>
      <h6>UC San Diego</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.utexas.edu/~huangqx/">
      <img class="people-pic" src="{{ "/static/img/people/qixing.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.utexas.edu/~huangqx/">Qixing Huang</a>
      <h6>UT Austin</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://mvig.sjtu.edu.cn/">
      <img class="people-pic" src="{{ "/static/img/people/cewu.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://mvig.sjtu.edu.cn/">Cewu Lu</a>
      <h6>Shanghai Jiao Tong University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://geometry.stanford.edu/member/guibas/">
      <img class="people-pic" src="{{ "/static/img/people/leonidas.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>
      <h6>Stanford University</h6>
    </div>
  </div>

</div>

<hr>

{% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>
{% endif %}

<!--
<div class="row">
  <div class="col-xs-12">
    <h2>Sponsors</h2>
  </div>
</div>
<a name="/sponsors"></a>
<div class="row">
  <div class="col-xs-12 sponsor">
    <a href="https://eccv2018.org/">
      <img src="{{ "/static/img/ico/eccv18.png" | prepend:site.baseurl }}">
    </a>
  </div>
</div>
<br>
-->
<br>

<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    <ol>
      <li>Su, H., Qi, C.R., Li, Y., Guibas, L.J.: Render for cnn: Viewpoint estimation in
          images using cnns trained with rendered 3d model views. In: Proceedings of the
          IEEE International Conference on Computer Vision. (2015) 2686–2694</li>
      <li>Chen, W., Wang, H., Li, Y., Su, H., Wang, Z., Tu, C., Lischinski, D., Cohen-Or, D.,
          Chen, B.: Synthesizing training images for boosting human 3d pose estimation. In:
          3D Vision (3DV), 2016 Fourth International Conference on, IEEE (2016) 479–488</li>
      <li>Toshev, A., Makadia, A., Daniilidis, K.: Shape-based object recognition in videos
          using 3d synthetic object models. In: Computer Vision and Pattern Recognition,
          2009. CVPR 2009. IEEE Conference on, IEEE (2009) 288–295</li>
      <li>Georgakis, G., Mousavian, A., Berg, A.C., Kosecka, J.: Synthesizing training data
          for object detection in indoor scenes. arXiv preprint arXiv:1702.07836 (2017)</li>
      <li>Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M.: The synthia
          dataset: A large collection of synthetic images for semantic segmentation of urban
          scenes. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
          Recognition. (2016) 3234–3243</li>
      <li>Choy, C.B., Xu, D., Gwak, J., Chen, K., Savarese, S.: 3d-r2n2: A unified approach
          for single and multi-view 3d object reconstruction. In: European Conference on
          Computer Vision, Springer (2016) 628–644</li>
      <li>Fan, H., Su, H., Guibas, L.: A point set generation network for 3d object reconstruction
          from a single image. In: Conference on Computer Vision and Pattern
          Recognition (CVPR). Volume 38. (2017)</li>
      <li>Tatarchenko, M., Dosovitskiy, A., Brox, T.: Octree generating networks: Efficient
          convolutional architectures for high-resolution 3d outputs. CoRR, abs/1703.09438
          (2017)</li>
      <li>Kar, A., Häne, C., Malik, J.: Learning a multi-view stereo machine. In: Advances
          in Neural Information Processing Systems. (2017) 364–375</li>
      <li>Byravan, A., Fox, D.: Se3-nets: Learning rigid body motion using deep neural networks.
          In: Robotics and Automation (ICRA), 2017 IEEE International Conference
          on, IEEE (2017) 173–180</li>
      <li>Schenck, C., Fox, D.: Reasoning about liquids via closed-loop simulation. arXiv
          preprint arXiv:1703.01656 (2017)</li>
      <li>Wu, J., Yildirim, I., Lim, J.J., Freeman, B., Tenenbaum, J.: Galileo: Perceiving
          physical object properties by integrating a physics engine with deep learning. In:
          Advances in neural information processing systems. (2015) 127–135</li>
      <li>Wu, J., Lu, E., Kohli, P., Freeman, B., Tenenbaum, J.: Learning to see physics
          via visual de-animation. In: Advances in Neural Information Processing Systems.
          (2017) 152–163</li>
      <li>Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in
          natural images. In: Proceedings of the IEEE Conference on Computer Vision and
          Pattern Recognition. (2016) 2315–2324</li>
      <li>Brodeur, S., Perez, E., Anand, A., Golemo, F., Celotti, L., Strub, F., Rouat,
          J., Larochelle, H., Courville, A.: HoME: A household multimodal environment.
          arXiv:1711.11017 (2017)</li>
      <li>Kolve, E., Mottaghi, R., Gordon, D., Zhu, Y., Gupta, A., Farhadi, A.: AI2-THOR:
          An interactive 3D environment for visual AI. arXiv:1712.05474 (2017)</li>
      <li>Wu, Y., Wu, Y., Gkioxari, G., Tian, Y.: Building generalizable agents with a
          realistic and rich 3D environment. arXiv:1801.02209 (2018)</li>
      <li>Yan, C., Misra, D., Bennnett, A., Walsman, A., Bisk, Y., Artzi, Y.: CHALET:
          Cornell house agent learning environment. arXiv:1801.07357 (2018)</li>
      <li>Savva, M., Chang, A.X., Dosovitskiy, A., Funkhouser, T., Koltun, V.: MINOS:
          Multimodal indoor simulator for navigation in complex environments.
          arXiv:1712.03931 (2017)</li>
      <li>Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sünderhauf, N., Reid,
          I., Gould, S., van den Hengel, A.: Vision-and-Language Navigation: Interpreting
          visually-grounded navigation instructions in real environments. In: CVPR. (2018)</li>
      <li>Zamir, A.R., Xia, F., He, J., Sax, S., Malik, J., Savarese, S.: Gibson Env: Real-world
          perception for embodied agents. In: CVPR. (2018)</li>
      <li>Zhu, Y., Mottaghi, R., Kolve, E., Lim, J.J., Gupta, A., Fei-Fei, L., Farhadi, A.:
          Target-driven visual navigation in indoor scenes using deep reinforcement learning.
          In: ICRA. (2017)</li>
      <li>Gupta, S., Davidson, J., Levine, S., Sukthankar, R., Malik, J.: Cognitive mapping
          and planning for visual navigation. In: CVPR. (2017)</li>
      <li>Chaplot, D.S., Sathyendra, K.M., Pasumarthi, R.K., Rajagopal, D., Salakhutdinov,
          R.: Gated-attention architectures for task-oriented language grounding. In: AAAI.
          (2018)</li>
      <li>Misra, D.K., Langford, J., Artzi, Y.: Mapping instructions and visual observations
          to actions with reinforcement learning. In: EMNLP. (2017)</li>
      <li>Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Embodied Question
          Answering. In: CVPR. (2018)</li>
      <li>Gordon, D., Kembhavi, A., Rastegari, M., Redmon, J., Fox, D., Farhadi, A.: IQA:
          Visual question answering in interactive environments. In: CVPR. (2018)</li>
      <li>Richter, S., Vineet, V., Roth, S., Koltun, V.: Playing for Data: Ground Truth from
          Computer Games. In ECCV (2016).</li>
    </ol>
  </div>
</div>
