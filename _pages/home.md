---
layout: project
urltitle:  "Visual Learning and Embodied Agents in Simulation Environments"
title: "Visual Learning and Embodied Agents in Simulation Environments"
categories: eccv, munich, germany, workshop, computer vision, embodied agents, visual learning, simulation environments, robotics, visual navigation, machine learning, natural language processing, reinforcement learning
permalink: /
favicon: /static/img/embodiedqa/favicon.png
bibtex: true
paper: true
---

<!-- In case, acks are needed, uncomment the following and place in the intro block -->

<!-- acknowledgements: "We are grateful to the developers of PyTorch for building an excellent framework.
We thank Yuxin Wu for help with the House3D environment.
This work was funded in part by NSF CAREER awards to DB and DP, ONR YIP awards
to DP and DB, ONR Grant N00014-14-1-0679 to DB, ONR Grant N00014-16-1-2713 to DP,
an Allen Distinguished Investigator award to DP from the Paul G. Allen Family Foundation,
Google Faculty Research Awards to DP and DB, Amazon Academic Research Awards to DP and DB,
AWS in Education Research grant to DB, and NVIDIA GPU donations to DB. The views and
conclusions contained herein are those of the authors and should not be interpreted as
necessarily representing the official policies or endorsements, either expressed
or implied, of the U.S. Government, or any sponsor." -->

<br>
<div class="row">
  <div class="col-xs-12">
    <center><h1>Visual Learning and Embodied Agents in Simulation Environments</h1></center>
    <center><h2>ECCV 2018 Workshop, Munich, Germany</h2></center>
    <center><span style="color:#e74c3c;font-weight:400;" id="intro">
      Sunday, 9th September, 08:45 AM to 06:00 PM, Room: TBD
    </span></center>
  </div>
</div>

<hr>

<br>
<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      Simulation environments are having a profound impact on computer vision and
      artificial intelligence (AI) research. Synthetic environments can be used to generate
      unlimited cheap, labeled data for training data-hungry <span style="font-weight:500;">visual learning</span> algorithms
      for perception tasks such as 3D pose estimation [1, 2], object detection and
      recognition [3, 4], semantic segmentation [5], 3D reconstruction [6-9], intuitive
      physics modeling [10-13] and text localization [14]. In addition, visually-realistic
      simulation environments designed for <span style="font-weight:500;">embodied agents</span> [15-21] have reignited interest
      in high-level AI tasks such as visual navigation [22, 23], natural language
      instruction following [20, 24, 25] and embodied question answering [26, 27]. This
      workshop will bring together researchers from computer vision, machine learning,
      natural language processing and robotics to examine the challenges and
      opportunities in this rapidly developing area - using simulation environments to
      develop intelligent embodied agents and other vision-based systems.
    </p>
  </div>
  <!-- <div class="col-xs-12">
    <span style="color:#e74c3c;font-weight:400;">Dec 2017</span> — <a target="_blank" href="https://github.com/facebookresearch/house3d">Code for 3D environments</a> is now available!
  </div> -->
</div> <br>   

<div class="row">
  <div class="col-xs-12">
    <h2>Call for papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      We solicit high-quality short (4-page) and long (8-page) paper submissions,
      which may also include a live demo. Demos will provide workshop participants
      with an important opportunity to interact with simulation environments, algorithms
      and agents, to better understand the strengths and limitations of current
      work. All submissions will be peer reviewed. Accepted submissions will be showcased
      in joint poster/demo sessions, with four paper submissions selected for
      spotlight oral presentation. Full papers will be posted on the workshop website
      but will not appear in the official proceedings. Submissions will be separated
      into two tracks. Papers and demos must be submitted to one track only.

      <h3>Visual Learning Track</h3><br>
      <ul>
        <li>Established computer vision tasks using synthetic data (e.g. 3D pose estimation,
            object recognition, object detection, semantic segmentation, text localization, single-image 3D reconstruction and indoor/outdoor scene understanding)</li>
        <li>Novel computer vision tasks using synthetic data</li>
        <li>Understanding physics from visual input</li>
        <li>Domain adaptation from simulators to the real world</li>
      </ul><br>

      <h3 id="dates">Embodied Agents Track</h3><br>
      <ul>
        <li>Embodied agents operating in simulation environments, including reinforcement
            learning and approaches that use mapping and planning</li>
        <li>Novel datasets / simulators / tasks for embodied agents</li>
        <li>Language-based command of embodied agents, including embodied question
            answering and / or dialog</li>
        <li>Photo-realistic simulations from reconstructed point clouds / 3D meshes</li>
        <li>Simulating interactions with objects in environments</li>
        <li>Domain adaptation for embodied agents</li>
        <li>Simulating people and environment changes</li>
      </ul>

    </p>
  </div>

  <!-- <div class="col-xs-12">
    <span style="color:#e74c3c;font-weight:400;">Dec 2017</span> — <a target="_blank" href="https://github.com/facebookresearch/house3d">Code for 3D environments</a> is now available!
  </div> -->
</div><br>

<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div><br>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>June 15th, 2018</td>
        </tr>
        <tr>
          <td>Final Decisions</td>
          <td>July 16th, 2018</td>
        </tr>
        <tr>
          <td>Camera-Ready deadline</td>
          <td>August 1st, 2018</td>
        </tr>
        <tr>
          <td>Worshop Date</td>
          <td>September 9th, 2018</td>
        </tr>
      </tbody>
    </table>
    <!-- <p>15th June : Submission Deadline</p>
    <p>16h July : Acceptance Notification</p>
    <p>1st August : Camera-ready deadline</p>
    <p>9th Septmeber : Workshop date</p> -->
    <!-- <ul>
      <li>15th June : Submission Deadline</li>
      <li>16h July : Acceptance Notification</li>
      <li>1st August : Camera-ready deadline</li>
      <li>9th Septmeber : Workshop date</li>
    </ul> -->
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <ul>
      <li>08:45 AM : Welcome and Introduction</li> 
      <li>09:00 AM : Speaker 1</li> 
      <li>09:25 AM : Speaker 2</li>
      <li>09:50 AM : Speaker 3</li>
      <li>10:15 AM : Coffee + Posters/Demos (Visual Learning Track)</li>
      <li>11:00 AM : Speaker 4</li>
      <li>11:25 AM : Speaker 5</li>
      <li>11:50 AM : Poster Spotlight Presentations (4 × 5 min)</li>
      <li>12:10 PM : Lunch</li>
      <li>01:30 PM : Speaker 6</li>
      <li>01:55 PM : Speaker 7</li>
      <li>02:20 PM : Speaker 8</li>
      <li>02:45 PM : Coffee + Posters/Demos (Embodied Agents Track)</li>
      <li>03:30 PM : Speaker 9</li>
      <li>03:55 PM : Speaker 10</li>
      <li>04:20 PM : Speaker 11</li>
      <li>04:45 PM : Panel Discussion</li>
      <li id="speakers">05:30 PM : Closing Remarks</li>
    </ul>
  </div>
</div>

<br>
<div class="row">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/jitendra.jpg" | prepend:site.baseurl }}">
    <p><b>Jitendra Malik</b> is currently the Arthur J. Chick Professor in the Department of Electrical Engineering and Computer Sciences. His research group has worked on many different topics in computer vision, computational modeling of human vision and computer graphics. Several well-known concepts and algorithms arose in this research, such as normalized cuts, high dynamic range imaging and R-CNN. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://people.eecs.berkeley.edu/~malik/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/vladlen.jpg" | prepend:site.baseurl }}">
    <p><b>Vladlen Koltun</b> is a Senior Principal Researcher and the director of the Intelligent Systems Lab at Intel. The lab is devoted to high-impact basic research on intelligent systems. He has published primarily in computer vision, machine learning, robotics, and graphics conferences. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://vladlen.info/">Webpage]</a></span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/devi.png" | prepend:site.baseurl }}">
    <p><b>Devi Parikh</b> (<b>Alt: Dhruv Batra</b>) is an Assistant Professor in the School of Interactive Computing at Georgia Tech and a Visiting Researcher at Facebook AI Research (FAIR). From 2013 to 2016 she was an Assistant Professor in the Bradley Department of Electrical and Computer Engineering at Virginia Tech. From 2009 to 2012 she was a Research Assistant Professor at Toyota Technological Institute at Chicago (TTIC). &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://www.cc.gatech.edu/~parikh/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/raia.jpg" | prepend:site.baseurl }}">
    <p><b>Raia Hadsell</b> is a research scientist on the Deep Learning team at DeepMind. She completed her PhD with Yann LeCun, at NYU, focused on machine learning using Siamese neural nets and on deep learning for mobile robots in the wild. Her thesis, 'Learning Long-range vision for offroad robots', was awarded the Outstanding Dissertation award in 2009. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://raiahadsell.com/index.html">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/lawson.jpg" | prepend:site.baseurl }}">
    <p><b>Lawson Wong</b> is a postdoctoral fellow at Brown University, working with Stefanie Tellex. He completed his Ph.D. in 2016 at the Massachusetts Institute of Technology, advised by Leslie Pack Kaelbling and Tomás Lozano-Pérez. His current research focuses on acquiring, representing, and estimating knowledge about the world that an autonomous robot may find useful. He was awarded a AAAI Robotics Student Fellowship in 2015. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://cs.brown.edu/~lwong5/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/abhinav.jpg" | prepend:site.baseurl }}">
    <p><b>Abhinav Gupta</b> is an assistant professor at Carnegie Mellon University. Prior to this, he was a post-doctoral fellow here working with Alyosha Efros and Martial Hebert. His research focuses on developing representation and reasoning approaches for deeper understanding of the scene, how linguistic information can be harnessed to efficiently learn how the world works and how are actions and objects related to each other. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://www.cs.cmu.edu/~abhinavg/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/matthias.jpg" | prepend:site.baseurl }}">
    <p><b>Matthias Nießner</b> (<b>Alt: Anton van den Hengel</b>) is heading the Visual Computing Lab at Technical University of Munich (TUM). He was a Visiting Assistant Professor at Stanford University from 2013 to 2017. Since 2017 he is Professor at TUM, where he is focusing on static and dynamic 3D reconstruction approaches with a strong focus on modern machine learning and optimization techniques. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://niessnerlab.org/index.html">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/sanja.jpg" | prepend:site.baseurl }}">
    <p><b>Sanja Fidler</b> (<b>Alt: Raquel Urtasun</b>) is an Assistant Professor at University of Toronto. Her main research interests are 2D and 3D object detection, particularly scalable multi-class detection, object segmentation and image labeling, and (3D) scene understanding. She is also interested in the interplay between language and vision. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://www.cs.utoronto.ca/~fidler/">Webpage</a>] </span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/dieter.jpg" | prepend:site.baseurl }}">
    <p><b>Dieter Fox</b> is a Professor in the Department of Computer Science and Engineering at the University of Washington. His research interests are in robotics, artificial intelligence, and state estimation. He is the head of the UW Robotics and State Estimation Lab RSE-Lab and currently serves as the academic PI of the Intel Science and Technology Center for Pervasive Computing ISTC-PC. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="https://homes.cs.washington.edu/~fox/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/alan.jpg" | prepend:site.baseurl }}">
    <p><b>Alan Yuille</b> is a Bloomberg Distinguished Professor of Cognitive Science and Computer Science at Johns Hopkins University. He directs the research group on Compositional Cognition, Vision, and Learning. He is affiliated with  the Center for Brains, Minds and Machines, and the NSF Expedition in Computing, Visual Cortex On Silicon. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://www.cs.jhu.edu/~ayuille/">Webpage</a>]</span></p>
  </div>
</div><br>

<div class="row"  id="organizers">
  <div class="col-md-12">
    <img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/people/boqing.jpg" | prepend:site.baseurl }}">
    <p><b>Boqing Gong</b> (<b>Alt: Peter Welinder</b>) is an Assistant Professor in Computer Science and CRCV (Center for Research in Computer Vision) at the University of Central Florida (UCF). His research lies at the intersection of machine learning and computer vision, and has been focusing on domain adaptation, zero-shot/transfer learning, and visual analytics of objects, scenes, attributes, and human activities. &nbsp;
    <span style="color:#1a1aff;font-weight:400;">[<a href="http://boqinggong.info/">Webpage</a>]</span></p>
  </div>
</div><br>


<div class="row">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-2">
    <a href="http://www.panderson.me/">
      <img class="people-pic" src="{{ "/static/img/people/peter.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.panderson.me/">Peter Anderson</a>
      <h6>Australian National University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://msavva.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/manolis.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://msavva.github.io/">Manolis Savva</a>
      <h6>Princeton University</h6>
    </div>
  </div>
  
  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/angel.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Princeton University</h6>
    </div>
  </div>
  
  <div class="col-xs-2">
    <a href="https://people.eecs.berkeley.edu/~sgupta/">
      <img class="people-pic" src="{{ "/static/img/people/saurabh.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://people.eecs.berkeley.edu/~sgupta/">Saurabh Gupta</a>
      <h6>UC Berkeley</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://cs.stanford.edu/~amirz/">
      <img class="people-pic" src="{{ "/static/img/people/amir.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://cs.stanford.edu/~amirz/">Amir R. Zamir</a>
      <h6>UC Berkeley, Stanford Univeristy</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cc.gatech.edu/~slee3191/">
      <img class="people-pic" src="{{ "/static/img/people/stefan.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cc.gatech.edu/~slee3191/">Stefan Lee</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>
</div>

<div class="row">
  <div class="col-xs-2">
    <a href="https://samyak-268.github.io">
      <img class="people-pic" src="{{ "/static/img/people/samyak.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://samyak-268.github.io">Samyak Datta</a>
      <h6>Georgia Tech</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://cs.stanford.edu/~ericyi/">
      <img class="people-pic" src="{{ "/static/img/people/li.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://cs.stanford.edu/~ericyi/">Li Yi</a>
      <h6>Stanford University</h6>
    </div>
  </div>
  
  <div class="col-xs-2">
    <a href="http://cseweb.ucsd.edu/~haosu/">
      <img class="people-pic" src="{{ "/static/img/people/hao.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://cseweb.ucsd.edu/~haosu/">Hao Su</a>
      <h6>UC San Diego</h6>
    </div>
  </div>
  
  <div class="col-xs-2">
    <a href="https://www.cs.utexas.edu/~huangqx/">
      <img class="people-pic" src="{{ "/static/img/people/qixing.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.utexas.edu/~huangqx/">Qixing Huang</a>
      <h6>UT Austin</h6>
    </div>
  </div>

  <div class="col-xs-2" id="sponsors">
    <a href="https://geometry.stanford.edu/member/guibas/">
      <img class="people-pic" src="{{ "/static/img/people/leonidas.jpg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibus</a>
      <h6>Stanford University</h6>
    </div>
  </div>

</div>

<hr>

{% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgements</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      {{ page.acknowledgements }}
    </p>
  </div>
</div>
{% endif %}

<div class="row">
  <div class="col-xs-12">
    <h2>Sponsors</h2>
  </div>
</div>
<a name="/sponsors"></a>
<div class="row">
  <div class="col-xs-12 sponsor">
    <img src="{{ "/static/img/ico/eccv18.png" | prepend:site.baseurl }}">
  </div>
</div>
<br>
<br>

<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    <ol>
      <li>Su, H., Qi, C.R., Li, Y., Guibas, L.J.: Render for cnn: Viewpoint estimation in
          images using cnns trained with rendered 3d model views. In: Proceedings of the
          IEEE International Conference on Computer Vision. (2015) 2686–2694</li>
      <li>Chen, W., Wang, H., Li, Y., Su, H., Wang, Z., Tu, C., Lischinski, D., Cohen-Or, D.,
          Chen, B.: Synthesizing training images for boosting human 3d pose estimation. In:
          3D Vision (3DV), 2016 Fourth International Conference on, IEEE (2016) 479–488</li>
      <li>Toshev, A., Makadia, A., Daniilidis, K.: Shape-based object recognition in videos
          using 3d synthetic object models. In: Computer Vision and Pattern Recognition,
          2009. CVPR 2009. IEEE Conference on, IEEE (2009) 288–295</li>
      <li>Georgakis, G., Mousavian, A., Berg, A.C., Kosecka, J.: Synthesizing training data
          for object detection in indoor scenes. arXiv preprint arXiv:1702.07836 (2017)</li>
      <li>Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M.: The synthia
          dataset: A large collection of synthetic images for semantic segmentation of urban
          scenes. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
          Recognition. (2016) 3234–3243</li>
      <li>Choy, C.B., Xu, D., Gwak, J., Chen, K., Savarese, S.: 3d-r2n2: A unified approach
          for single and multi-view 3d object reconstruction. In: European Conference on
          Computer Vision, Springer (2016) 628–644</li>
      <li>Fan, H., Su, H., Guibas, L.: A point set generation network for 3d object recon-
          struction from a single image. In: Conference on Computer Vision and Pattern
          Recognition (CVPR). Volume 38. (2017)</li>
      <li>Tatarchenko, M., Dosovitskiy, A., Brox, T.: Octree generating networks: Efficient
          convolutional architectures for high-resolution 3d outputs. CoRR, abs/1703.09438
          (2017)</li>
      <li>Kar, A., Häne, C., Malik, J.: Learning a multi-view stereo machine. In: Advances
          in Neural Information Processing Systems. (2017) 364–375</li>
      <li>Byravan, A., Fox, D.: Se3-nets: Learning rigid body motion using deep neural net-
          works. In: Robotics and Automation (ICRA), 2017 IEEE International Conference
          on, IEEE (2017) 173–180</li>
      <li>Schenck, C., Fox, D.: Reasoning about liquids via closed-loop simulation. arXiv
          preprint arXiv:1703.01656 (2017)</li>
      <li>Wu, J., Yildirim, I., Lim, J.J., Freeman, B., Tenenbaum, J.: Galileo: Perceiving
          physical object properties by integrating a physics engine with deep learning. In:
          Advances in neural information processing systems. (2015) 127–135</li>
      <li>Wu, J., Lu, E., Kohli, P., Freeman, B., Tenenbaum, J.: Learning to see physics
          via visual de-animation. In: Advances in Neural Information Processing Systems.
          (2017) 152–163</li>
      <li>Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in
          natural images. In: Proceedings of the IEEE Conference on Computer Vision and
          Pattern Recognition. (2016) 2315–2324</li>
      <li>Brodeur, S., Perez, E., Anand, A., Golemo, F., Celotti, L., Strub, F., Rouat,
          J., Larochelle, H., Courville, A.: HoME: A household multimodal environment.
          arXiv:1711.11017 (2017)</li>
      <li>Kolve, E., Mottaghi, R., Gordon, D., Zhu, Y., Gupta, A., Farhadi, A.: AI2-THOR:
          An interactive 3D environment for visual AI. arXiv:1712.05474 (2017)</li>
      <li>Wu, Y., Wu, Y., Gkioxari, G., Tian, Y.: Building generalizable agents with a
          realistic and rich 3D environment. arXiv:1801.02209 (2018)</li>
      <li>Yan, C., Misra, D., Bennnett, A., Walsman, A., Bisk, Y., Artzi, Y.: CHALET:
          Cornell house agent learning environment. arXiv:1801.07357 (2018)</li>
      <li>Savva, M., Chang, A.X., Dosovitskiy, A., Funkhouser, T., Koltun, V.: MI-
          NOS: Multimodal indoor simulator for navigation in complex environments.
          arXiv:1712.03931 (2017)</li>
      <li>Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sünderhauf, N., Reid,
          I., Gould, S., van den Hengel, A.: Vision-and-Language Navigation: Interpreting
          visually-grounded navigation instructions in real environments. In: CVPR. (2018)</li>
      <li>Zamir, A.R., Xia, F., He, J., Sax, S., Malik, J., Savarese, S.: Gibson Env: Real-
          world perception for embodied agents. In: CVPR. (2018)</li>
      <li>Zhu, Y., Mottaghi, R., Kolve, E., Lim, J.J., Gupta, A., Fei-Fei, L., Farhadi, A.:
          Target-driven visual navigation in indoor scenes using deep reinforcement learning.
          In: ICRA. (2017)</li>
      <li>Gupta, S., Davidson, J., Levine, S., Sukthankar, R., Malik, J.: Cognitive mapping
          and planning for visual navigation. In: CVPR. (2017)</li>
      <li>Chaplot, D.S., Sathyendra, K.M., Pasumarthi, R.K., Rajagopal, D., Salakhutdinov,
          R.: Gated-attention architectures for task-oriented language grounding. In: AAAI.
          (2018)</li>
      <li>Misra, D.K., Langford, J., Artzi, Y.: Mapping instructions and visual observations
          to actions with reinforcement learning. In: EMNLP. (2017)</li>
      <li>Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Embodied Question
          Answering. In: CVPR. (2018)</li>
      <li>Gordon, D., Kembhavi, A., Rastegari, M., Redmon, J., Fox, D., Farhadi, A.: IQA:
          Visual question answering in interactive environments. In: CVPR. (2018)</li>
    </ol>
  </div>
</div>
